{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFuRj2qPr_i3"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/Torsion-Audio/Scyclone/main/assets/pictures/interface.png\" width=\"100%\"/>\n",
        "\n",
        "__Scyclone__ is an audio plugin developed by **Torsion Audio** that utilizes neural timbre transfer technology to offer a new approach to audio production. The plugin builds upon [RAVE](https://github.com/acids-ircam/RAVE) [Antoine Cailon et. al], a realtime audio variational auto encoder, facilitating neural timbre transfer in both single and couple inference mode.\n",
        "\n",
        "This enables a new artificial layering technique to be applied on the incoming signal in creating richer drum pallets, fuller atmospheres or simply transferring the timbre of the raw signal to another sound pallet. To further control the behaviour and production of the neural networks, we have internally equipped the plugin with signal processing modules allowing the user to shape, control and embellish the source and target timbres in a distinct manner.\n",
        "\n",
        "Scyclone comes with two pre-trained models, **funk_drums** trained on four hours of data inspired by the captivating sounds of vintage drum-breaks and **Djembe**, trained on five hours of carefully compiled Djembe dataset.\n",
        "\n",
        "\n",
        "This notebook allows you to train your own models on customized datasets to use as presets inside the plugin. Although we encorage the interested reader to study the RAVE [article](https://arxiv.org/pdf/2111.05011.pdf) and visit the well-written [github](https://github.com/acids-ircam/RAVE) repository, in this notebook we have gathered all the necessary information required to successfully train a model without holding any former knowledge. The notebook is strcutured as follows:\n",
        "\n",
        "\\\n",
        "\n",
        "\n",
        "\n",
        "*   **Pre-requsites**\n",
        "*   **Dataset**\n",
        "*   **Training**\n",
        "*   **Export**\n",
        "\n",
        "\n",
        "\\\n",
        "\n",
        "\n",
        "There are several options available for training machine learning models, including both local machines and cloud platforms. For simplicity's sake, we have chosen Google Colab as it offers a convenient all-in-one solution. It's important to mention that our selection of Google Colab does not imply any endorsement of Google or any other cloud computing service provider.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/Torsion-Audio/Scyclone/main/assets/pictures/logo.png\" alt=\"alt\" width=\"1%\"/>  **`Torsion Audio`**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VqVmieuB9DT"
      },
      "source": [
        "# 1. Pre-requisites\n",
        "\n",
        "To upload your customized dataset and save training checkpoints, you can use Google Drive. If you do not have a Google Drive account yet, follow the instructions on this link:\n",
        "> https://accounts.google.com/AccountChooser?service=writely\n",
        "\n",
        "Once you have set-up an account, log-in to Colab and run the cell to mount the drive on this session. To run a cell, simply press the *play* button on the left-handside of each cell. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFDBsx3DCfI6",
        "outputId": "d746b190-7e09-4341-c67a-adca23abcc02",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at content\n"
          ]
        }
      ],
      "source": [
        "#@title 1.1 Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('content')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZndpYOqDqxB"
      },
      "source": [
        "\n",
        "\n",
        "Model training is a resource-intensive process that demands significant time and computational power. Machine learning practitioners often rely on the accelerated computing capabilities of GPUs to expedite model acquisition and optimization. Google Colab addresses this need by providing access to GPUs, allowing users to leverage their computational capabilities. The availability and duration of GPU access in Google Colab may vary depending on the user's subscription method or plan.\n",
        "\n",
        "\\\n",
        "\n",
        "> ### 1.2 Purchase Compute Units\n",
        "To access faster GPUs, we need to purchase compute units. Compute units is a currency used by Google Colab to calculate GPU utilisation. To purchase compute units:\n",
        "```\n",
        "1. Click on RAM/Disk button on the top right-hand side of this notebook\n",
        "2. Under \"Resources\", click on \"Learn more.\"\n",
        "3. Follow the instruction and purchase computation units\n",
        "```\n",
        "To provide you with insights, here is the estimate amount of compute units spent to train the **funk_Drums** preset in Scyclone:\n",
        "```\n",
        "GPU Type:     A100\n",
        "C Units:      750\n",
        "Usage rate:   ~13 C Units/hr\n",
        "Runtime:      60hrs\n",
        "Dataset size: 3:31:00\n",
        "```\n",
        "\n",
        "\\\n",
        "Please note that the GPU type and dataset size influences the **usage rate** and the **compute units** required to train a model. You can also initiate a training without purchasing any compute units. In this case Google assigns you an interruptable medium-class GPU which greatly influences the time it takes to obtain an optimal sounding model. \n",
        "\n",
        "Once you have purchased enough compute units, you are all set to move on to the next step. You can see your available compute units under **Recourses** on the top right-hand side of this notebook.We will now install the necessary libraries and dependencies required for the training. You can **Install Dependencies** by running it's respective cell below. This might take several minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Rlc6DI7_Epj",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 1.3 Install Dependencies\n",
        "\n",
        "!curl -L https://repo.anaconda.com/miniconda/Miniconda3-py39_4.12.0-Linux-x86_64.sh -o miniconda.sh\n",
        "!chmod +x miniconda.sh\n",
        "!sh miniconda.sh -b -p /content/miniconda\n",
        "!/content/miniconda/bin/pip install --quiet acids-rave==2.1.7 #test without foring the version\n",
        "!/content/miniconda/bin/pip install --quiet pytorch_lightning==1.9.0\n",
        "!/content/miniconda/bin/pip install cached-conv==2.4.1\n",
        "!/content/miniconda/bin/pip install onnx\n",
        "!/content/miniconda/bin/pip install --quiet --upgrade ipython ipykernel\n",
        "!/content/miniconda/bin/conda install ffmpeg -y\n",
        "!/content/miniconda/bin/pip install effortless_config\n",
        "!/content/miniconda/bin/apt-get install unzip\n",
        "#!git clone https://github.com/Torsion-Audio/Scyclone-AI/tree/main\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLsBN-Kg3zB9"
      },
      "source": [
        "# 2. Dataset\n",
        "\n",
        "\n",
        "> ### 2.1 Compiling Datasets\n",
        "Percussive sound datasets exhibit greater compatibility with RAVE timbre transfer models, based on our extensive experience in the field. In order to facilitate effective model learning from shared sound structures, it is prudent to ensure alignment of sound characteristics during dataset creation. This can be accomplished by focusing on specific percussion instruments or instrument groups. Please note that this shouldn't discourage you from experimenting with different datasets other than tbose with percussive characteristcs.\n",
        "\n",
        "> The acquisition of data can be achieved through the process of sound recording or by gathering samples from production libraries or musical compositions. These samples may consist of individual percussive sounds (one-shots) or longer recordings (loops and musical pieces). To ensure optimal training outcomes, it is advisable to curate a well-balanced collection of sounds within the dataset, while simultaneously avoiding recordings that possess excessive room reverberations. \n",
        "\n",
        "> Furthermore, the size of the dataset should be proportional to its diversity. For datasets comprising single percussion instruments, a minimum duration of 3 hours is recommended, whereas larger sound palettes should encompass at least 6 hours of audio material.\n",
        "\n",
        "> ### 2.2 Upload Dataset\n",
        "Once you have your dataset zipped and stored on your local drive, you can easily upload it to Google Drive by following these steps:\n",
        "```\n",
        "1. On the top left-hand side of this page, click on the \"folder\" icon and open the browser\n",
        "2. Navigate to /content/MyDrive/\n",
        "3. Right-click on \"MyDrive\" folder and create a \"New Folder\"\n",
        "4. Rename the folder to \"scyclone\"\n",
        "5. In the same directory, create two new folders and rename them to \"dataset\" and \"ckpt\"\n",
        "6. Drag and Drop the .zip dataset into the \"dataset\" folder\n",
        "```\n",
        "This might take several minutes based on the size of your dataset. You'll now the upload has been successful onces the **blue circular progress bar** vanishes on the bottom-right of the browser.\n",
        "\n",
        "\n",
        "\n",
        "> ### 2.3 Unzip Data\n",
        "To unzip you need to provide the path to the **data_zip** and **dataset_dir**. Inside the Scyclone directory, create a folder and rename it to **dataset_unzip**. The paths should look like this now:\n",
        "```\n",
        "data_zip:     /content/content/MyDrive/scyclone/dataset/NameOfYourDataset.zip\n",
        "dataset_dir:  /content/content/MyDrive/scyclone/dataset/\n",
        "```\n",
        "Copy and paste the directories to the **Unzip data** cell and run it. This might also take several minutes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Yqtbxi4q02M",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Unzip data\n",
        "import os\n",
        "\n",
        "os.mkdir(\"/content/content/MyDrive/scyclone/dataset/dataset_unzip\") \n",
        "\n",
        "data_zip = \"\"     #@param {type:\"string\"}\n",
        "dataset_dir = \"\"  #@param {type:\"string\"}\n",
        "\n",
        "%cd dataset_dir\n",
        "\n",
        "!unzip $data_zip -d dataset_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71srGmdjxGIB"
      },
      "source": [
        "# 3. Training\n",
        "\n",
        "> RAVE comes with different architectures and training methods. In our experiments we realised **v1** and **onnx** to be more stable and suitable for our use case in comparison to other architectures. You are welcome to experiment with the architectures and configurations as you see fit. However, please consider that configurations other that the ones mentioned below might lead to an unsuitable outcome in respect to Scyclone.\n",
        "```\n",
        "1. Set the name of the training. This could be the preset name\n",
        "2. Set the dataset_dir to the directory where you have unzipped the dataset\n",
        "3. Set the save_dir to the directory where you want to save the checkpoints\n",
        "4. Architecture allows you to choose the RAVE architecture you are willing to train. We recommend v1 or onnx\n",
        "5. Currently SCYCLONE handles 48kHz of sampling rate. Therefore, we set the \"sr\" to 48000\n",
        "```\n",
        "The variables in the **Train** cell should look similar to:\n",
        "```\n",
        "name = \"preset_name\"          \n",
        "dataset_dir = \"./content/content/MyDrive/scyclone/dataset/dataset_unzip/\"    \n",
        "save_dir = \"./content/content/MyDrive/scyclone/ckpt/\"       \n",
        "architecture = \"v1\"\n",
        "sr = 48000   \n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> Run the cell to initiate the training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZ6ECduhA2_Q",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Train\n",
        "\n",
        "\n",
        "name = \"\"           #@param {type:\"string\"}\n",
        "dataset_dir = \"\"    #@param {type:\"string\"}\n",
        "save_dir = \"\"       #@param {type:\"string\"}\n",
        "architecture = \"v1\" #@param [\"v1\"]\n",
        "sr = 48000          #@param [48000]\n",
        "\n",
        "#set the architecture\n",
        "if architecture == 'v1':\n",
        "  architecture = '/content/Scyclone-AI/architectures/scyclone-config-v1.gin'\n",
        "\n",
        "\n",
        "%cd /content/\n",
        "!mkdir dataset\n",
        "%cd $save_dir\n",
        "preprocessed_dataset = \"/content/dataset\"\n",
        "\n",
        "\n",
        "!/content/miniconda/bin/rave preprocess --input_path $dataset_dir --output_path $preprocessed_dataset --sampling_rate $sr\n",
        "!/content/miniconda/bin/rave train --config $architecture --db_path $preprocessed_dataset --name $name --override LATENT_SIZE=16 --override CAPACITY=32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaBQa7Lh1Yqc"
      },
      "source": [
        "If the training was interrupted or you had to pause the training, you can use the **Resume** cell to continue from the last checkpoint.\n",
        "On that end, please ensure that all parameters and hyperparameters in the **Resume** cell are identical to the ones in the **Train** cell. \n",
        "Supposing:\n",
        "\n",
        "**name = funk_drums** \n",
        "\n",
        "Checkpoints should be saved like this:\n",
        "\n",
        "\n",
        "```\n",
        "./path/to/checkpoint_folder/runs/funk_drums_123456/version_0/checkpoints/last.ckpt\n",
        "./path/to/checkpoint_folder/runs/funk_drums_123456/version_0/checkpoints/last-v1.ckpt\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "The numbers next to the name in the directory path are randomly generated identifiers and could differ to the ones above. Copy the path to the **last-v1.ckpt** or **last.ckpt** and paste it infront of **resume_dir** and run the cell. You will also recieve a new **version** number everytime you continue the interrupted training; where the new checkpoints will be saved.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "s3ne3LYOan78"
      },
      "outputs": [],
      "source": [
        "#@title Resume \n",
        "\n",
        "name = \"\"           #@param {type:\"string\"}\n",
        "dataset_dir = \"\"    #@param {type:\"string\"}\n",
        "save_dir = \"\"       #@param {type:\"string\"}\n",
        "architecture = \"v1\" #@param [\"v1\"]\n",
        "sr = 48000       #@param [48000]\n",
        "resume_dir = \"\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "if architecture == 'v1':\n",
        "  architecture = '/content/Scyclone-AI/architectures/scyclone-config-v1.gin'\n",
        "\n",
        "\n",
        "%cd /content/\n",
        "!mkdir dataset\n",
        "%cd $save_dir\n",
        "preprocessed_dataset = \"/content/dataset\"\n",
        "\n",
        "\n",
        "# !/content/miniconda/bin/rave train --helpfull\n",
        "!/content/miniconda/bin/rave preprocess --input_path $dataset_dir --output_path $preprocessed_dataset --sampling_rate $sr\n",
        "!/content/miniconda/bin/rave train --config $architecture --db_path $preprocessed_dataset --name $name --ckpt $resume_dir --override LATENT_SIZE=16 --override CAPACITY=32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYIsWyI8pnmk"
      },
      "source": [
        "# 3.1 Training done?\n",
        "\n",
        "> There are numerous factors determining the required number of iterations to obtain an acceptable sounding model; discussing which is beyond the scope of this notebook. However, to provide you with some insights, here are sizes and the number of epochs of two models we trained:\n",
        "```\n",
        "name:    FUNK_DRUMS\n",
        "size:    3:31:00 hrs\n",
        "epochs:  ~6360\n",
        "Iters:   ~3.5 Million\n",
        "```\n",
        "```\n",
        "name:   DJEMBE\n",
        "size:   3:48:00 hrs\n",
        "epochs: ~5200\n",
        "Iters:  ~3 Million\n",
        "```\n",
        "\n",
        "> On that end, feel free to experiment with the number of iterations meaning to train the model for more or less iterations as the ones mentioned above. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC5hiF8SyDWV"
      },
      "source": [
        "# 4. Export\n",
        "\n",
        "Once the training is finished, copy the path to run directory and paste it in front of **run_dir** below and run the cell. The path should look similar to:\n",
        "\n",
        "\n",
        "```\n",
        "./path/to/checkpoint_folder/runs/funk_drums_123456\n",
        "\n",
        "```\n",
        "\n",
        "If you have experienced a disconnection from the runtime, it is necessary to remount Google Drive and reinstall the dependencies that are essential for exporting an **.onnx** model and performing the conversion to **.ort** format."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4.0 Remount Google Drive and reinstall dependencies\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('content')\n",
        "\n",
        "!curl -L https://repo.anaconda.com/miniconda/Miniconda3-py39_4.12.0-Linux-x86_64.sh -o miniconda.sh\n",
        "!chmod +x miniconda.sh\n",
        "!sh miniconda.sh -b -p /content/miniconda\n",
        "!/content/miniconda/bin/pip install --quiet acids-rave==2.1.7 #test without foring the version\n",
        "!/content/miniconda/bin/pip install --quiet pytorch_lightning==1.9.0\n",
        "!/content/miniconda/bin/pip install cached-conv==2.4.1\n",
        "!/content/miniconda/bin/pip install onnx\n",
        "!/content/miniconda/bin/pip install --quiet --upgrade ipython ipykernel\n",
        "!/content/miniconda/bin/conda install ffmpeg -y\n",
        "!/content/miniconda/bin/pip install effortless_config\n",
        "!git clone https://github.com/Torsion-Audio/Scyclone-AI/tree/main"
      ],
      "metadata": {
        "cellView": "form",
        "id": "iRqNT8xXN5C0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOxU6HKzQ3UM",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 4.1 Export Model\n",
        "import os\n",
        "\n",
        "#Export .onnx\n",
        "run_dir = \"/content/content/MyDrive/RAVE_TRAINING/onnx/jungle/runs/jungle_final_917935e402/runs/jungle_final_917935e402\" #@param {type:\"string\"}\n",
        "!/content/miniconda/bin/rave export_onnx --run $run_dir\n",
        "\n",
        "\n",
        "#install .onnx required for ort conversion\n",
        "!pip install onnxruntime==1.14.1\n",
        "!pip install onnx==1.13.1\n",
        "\n",
        "\n",
        "#paths for onnx to ort conversion\n",
        "model_name = run_dir.split('/')[-1]\n",
        "onnx_model_path = os.path.join(run_dir, model_name + '.onnx')\n",
        "ort_save_path  = run_dir\n",
        "\n",
        "\n",
        "# Create a directory to hold the ONNX model\n",
        "!mkdir -p \"$ort_save_path\"\n",
        "!python -m onnxruntime.tools.convert_onnx_models_to_ort \"$onnx_model_path\" --enable_type_reduction\n",
        "\n",
        "\n",
        "print('model exported succesfully')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4.2 Download Model\n",
        "\n",
        "from google.colab import files\n",
        "files.download(ort_save_path + \"/{}.ort\".format(model_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "cellView": "form",
        "id": "xmahno7v3_RY",
        "outputId": "ddb56e30-5771-47b2-b3b7-1f45ba916686"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8bdae03d-0c32-456e-b512-c49628be605e\", \"jungle_final_917935e402.ort\", 16575968)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H3FxRBE4KI2"
      },
      "source": [
        "You have now successfully exported the model!\n",
        "<img src=\"https://raw.githubusercontent.com/Torsion-Audio/Scyclone-AI/tree/main/assets/load_model.png\" width=\"100%\"/>\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "1. Save the .ort file to your local drive\n",
        "2. Open Scyclone\n",
        "3. Hover over one of the network nodes and select the preset loader icon on the network arm\n",
        "4. Select the trained .ort model and click \"open\"\n",
        "5. Now you have the model imported and it's ready for synthesis!\n",
        "\n",
        "```\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}